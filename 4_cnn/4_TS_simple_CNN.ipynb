{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cYyjhxRnF2x0",
    "outputId": "c7906e55-7a8e-4cb0-9944-8cc8431024ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "\n",
    "import seaborn as sns\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6r3H93DHoJ_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "def create_Xt_Yt(X, y, percentage=0.9):\n",
    "    p = int(len(X) * percentage)\n",
    "    X_train = X[0:p]\n",
    "    Y_train = y[0:p]\n",
    "     \n",
    "    X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
    " \n",
    "    X_test = X[p:]\n",
    "    Y_test = y[p:]\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def remove_nan_examples(data):\n",
    "    newX = []\n",
    "    for i in range(len(data)):\n",
    "        if np.isnan(data[i]).any() == False:\n",
    "            newX.append(data[i])\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xMybfhoF4zg"
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_csv('./AAPL1216.csv')[::-1]\n",
    "\n",
    "## Conv기반 windowing ->\n",
    "openp = data_original['Open'].tolist()\n",
    "highp = data_original['High'].tolist()\n",
    "lowp = data_original['Low'].tolist()\n",
    "closep = data_original['Adj Close'].tolist()\n",
    "volumep = data_original['Volume'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5sAJwS2GuV9"
   },
   "outputs": [],
   "source": [
    "WINDOW = 30\n",
    "EMB_SIZE = 5\n",
    "STEP = 1\n",
    "FORECAST = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5sAJwS2GuV9"
   },
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "#normalize every dimension of time window independently\n",
    "for i in range(0, len(data_original), STEP): \n",
    "    try:\n",
    "        o = openp[i:i+WINDOW]\n",
    "        h = highp[i:i+WINDOW]\n",
    "        l = lowp[i:i+WINDOW]\n",
    "        c = closep[i:i+WINDOW]\n",
    "        v = volumep[i:i+WINDOW]\n",
    "\n",
    "        o = (np.array(o) - np.mean(o)) / np.std(o)\n",
    "        h = (np.array(h) - np.mean(h)) / np.std(h)\n",
    "        l = (np.array(l) - np.mean(l)) / np.std(l)\n",
    "        c = (np.array(c) - np.mean(c)) / np.std(c)\n",
    "        v = (np.array(v) - np.mean(v)) / np.std(v)\n",
    "\n",
    "        x_i = closep[i:i+WINDOW]\n",
    "        y_i = closep[i+WINDOW+FORECAST]  \n",
    "\n",
    "        #다음날의 주가의 오름 혹은 내림을 예측하고 싶기 때문에 단일 디멘젼에서의 변화를 classify \n",
    "        last_close = x_i[-1]\n",
    "        next_close = y_i\n",
    "\n",
    "        if last_close < next_close:\n",
    "            y_i = [1, 0]\n",
    "        else:\n",
    "            y_i = [0, 1] \n",
    "\n",
    "        x_i = np.column_stack((o, h, l, c, v))\n",
    "\n",
    "    except Exception as e:\n",
    "        break\n",
    "\n",
    "    X.append(x_i)\n",
    "    Y.append(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kfbp_zipHy6a"
   },
   "outputs": [],
   "source": [
    "X, Y = np.array(X), np.array(Y)\n",
    "X_train, X_test, Y_train, Y_test = create_Xt_Yt(X, Y)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], EMB_SIZE))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], EMB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "kibaEJ-qH1FP",
    "outputId": "923f638d-a344-426b-d4f6-e1d5f0b5dd94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yckim\\Anaconda3\\envs\\TS_sample2\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(30, 5), filters=16, kernel_size=4, padding=\"same\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\yckim\\Anaconda3\\envs\\TS_sample2\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=8, kernel_size=4, padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(input_shape = (WINDOW, EMB_SIZE),\n",
    "                        nb_filter=16,\n",
    "                        filter_length=4,\n",
    "                        border_mode='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Conv1D\n",
    "model.add(Convolution1D(nb_filter=8,\n",
    "                        filter_length=4,\n",
    "                        border_mode='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "\n",
    "model.add(Dense(2)) # up/down\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "opt = Nadam(lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GP-7F5VXH4i1"
   },
   "outputs": [],
   "source": [
    "# LR스케쥴러\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.9, patience=30, min_lr=0.000001, verbose=1)\n",
    "# save model\n",
    "checkpointer = ModelCheckpoint(filepath=\"lolkek.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7i-dL6jYH8s0",
    "outputId": "b4dd6268-b152-4285-bedf-4c0081502d8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yckim\\Anaconda3\\envs\\TS_sample2\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 877 samples, validate on 98 samples\n",
      "Epoch 1/100\n",
      "877/877 [==============================] - 1s 1ms/step - loss: 0.8143 - accuracy: 0.5120 - val_loss: 0.7066 - val_accuracy: 0.4592\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70659, saving model to lolkek.hdf5\n",
      "Epoch 2/100\n",
      "877/877 [==============================] - 0s 82us/step - loss: 0.7542 - accuracy: 0.5325 - val_loss: 0.7142 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.70659\n",
      "Epoch 3/100\n",
      "128/877 [===>..........................] - ETA: 0s - loss: 0.8168 - accuracy: 0.4922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yckim\\Anaconda3\\envs\\TS_sample2\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877/877 [==============================] - 0s 83us/step - loss: 0.7327 - accuracy: 0.5439 - val_loss: 0.7159 - val_accuracy: 0.4694\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.70659\n",
      "Epoch 4/100\n",
      "877/877 [==============================] - 0s 80us/step - loss: 0.7296 - accuracy: 0.5393 - val_loss: 0.7137 - val_accuracy: 0.4898\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.70659\n",
      "Epoch 5/100\n",
      "877/877 [==============================] - 0s 92us/step - loss: 0.6994 - accuracy: 0.5644 - val_loss: 0.7109 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.70659\n",
      "Epoch 6/100\n",
      "877/877 [==============================] - 0s 96us/step - loss: 0.6981 - accuracy: 0.5564 - val_loss: 0.7093 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.70659\n",
      "Epoch 7/100\n",
      "877/877 [==============================] - 0s 94us/step - loss: 0.6921 - accuracy: 0.5701 - val_loss: 0.7131 - val_accuracy: 0.4694\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.70659\n",
      "Epoch 8/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6857 - accuracy: 0.5667 - val_loss: 0.7152 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.70659\n",
      "Epoch 9/100\n",
      "877/877 [==============================] - 0s 108us/step - loss: 0.6892 - accuracy: 0.5599 - val_loss: 0.7148 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.70659\n",
      "Epoch 10/100\n",
      "877/877 [==============================] - 0s 119us/step - loss: 0.7142 - accuracy: 0.5405 - val_loss: 0.7025 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.70659 to 0.70251, saving model to lolkek.hdf5\n",
      "Epoch 11/100\n",
      "877/877 [==============================] - 0s 113us/step - loss: 0.6833 - accuracy: 0.5792 - val_loss: 0.6990 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.70251 to 0.69901, saving model to lolkek.hdf5\n",
      "Epoch 12/100\n",
      "877/877 [==============================] - 0s 99us/step - loss: 0.6864 - accuracy: 0.5827 - val_loss: 0.7088 - val_accuracy: 0.4796\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69901\n",
      "Epoch 13/100\n",
      "877/877 [==============================] - 0s 127us/step - loss: 0.6662 - accuracy: 0.6226 - val_loss: 0.7002 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.69901\n",
      "Epoch 14/100\n",
      "877/877 [==============================] - 0s 128us/step - loss: 0.6739 - accuracy: 0.5781 - val_loss: 0.7098 - val_accuracy: 0.4694\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.69901\n",
      "Epoch 15/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6820 - accuracy: 0.5690 - val_loss: 0.7093 - val_accuracy: 0.4796\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.69901\n",
      "Epoch 16/100\n",
      "877/877 [==============================] - 0s 102us/step - loss: 0.6744 - accuracy: 0.5724 - val_loss: 0.7050 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.69901\n",
      "Epoch 17/100\n",
      "877/877 [==============================] - 0s 103us/step - loss: 0.6694 - accuracy: 0.5827 - val_loss: 0.6986 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.69901 to 0.69861, saving model to lolkek.hdf5\n",
      "Epoch 18/100\n",
      "877/877 [==============================] - 0s 98us/step - loss: 0.6749 - accuracy: 0.5747 - val_loss: 0.6963 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.69861 to 0.69630, saving model to lolkek.hdf5\n",
      "Epoch 19/100\n",
      "877/877 [==============================] - 0s 92us/step - loss: 0.6592 - accuracy: 0.6169 - val_loss: 0.6921 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.69630 to 0.69205, saving model to lolkek.hdf5\n",
      "Epoch 20/100\n",
      "877/877 [==============================] - 0s 90us/step - loss: 0.6742 - accuracy: 0.5815 - val_loss: 0.6881 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.69205 to 0.68810, saving model to lolkek.hdf5\n",
      "Epoch 21/100\n",
      "877/877 [==============================] - 0s 112us/step - loss: 0.6796 - accuracy: 0.5735 - val_loss: 0.6810 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.68810 to 0.68100, saving model to lolkek.hdf5\n",
      "Epoch 22/100\n",
      "877/877 [==============================] - 0s 105us/step - loss: 0.6659 - accuracy: 0.5918 - val_loss: 0.6962 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68100\n",
      "Epoch 23/100\n",
      "877/877 [==============================] - 0s 115us/step - loss: 0.6775 - accuracy: 0.5895 - val_loss: 0.7004 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68100\n",
      "Epoch 24/100\n",
      "877/877 [==============================] - 0s 109us/step - loss: 0.6607 - accuracy: 0.6032 - val_loss: 0.6935 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68100\n",
      "Epoch 25/100\n",
      "877/877 [==============================] - 0s 108us/step - loss: 0.6643 - accuracy: 0.6089 - val_loss: 0.6918 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.68100\n",
      "Epoch 26/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6688 - accuracy: 0.6055 - val_loss: 0.6914 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.68100\n",
      "Epoch 27/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6673 - accuracy: 0.5998 - val_loss: 0.6912 - val_accuracy: 0.5204\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.68100\n",
      "Epoch 28/100\n",
      "877/877 [==============================] - 0s 111us/step - loss: 0.6566 - accuracy: 0.6066 - val_loss: 0.6951 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68100\n",
      "Epoch 29/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.6619 - accuracy: 0.5918 - val_loss: 0.6916 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.68100\n",
      "Epoch 30/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.6497 - accuracy: 0.6192 - val_loss: 0.6893 - val_accuracy: 0.5204\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68100\n",
      "Epoch 31/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6627 - accuracy: 0.6043 - val_loss: 0.6825 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.68100\n",
      "Epoch 32/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.6522 - accuracy: 0.6203 - val_loss: 0.6825 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68100\n",
      "Epoch 33/100\n",
      "877/877 [==============================] - 0s 104us/step - loss: 0.6545 - accuracy: 0.6066 - val_loss: 0.6826 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68100\n",
      "Epoch 34/100\n",
      "877/877 [==============================] - 0s 95us/step - loss: 0.6462 - accuracy: 0.6169 - val_loss: 0.6880 - val_accuracy: 0.5204\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68100\n",
      "Epoch 35/100\n",
      "877/877 [==============================] - 0s 96us/step - loss: 0.6438 - accuracy: 0.6249 - val_loss: 0.6833 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68100\n",
      "Epoch 36/100\n",
      "877/877 [==============================] - 0s 111us/step - loss: 0.6360 - accuracy: 0.6283 - val_loss: 0.6823 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68100\n",
      "Epoch 37/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6531 - accuracy: 0.6123 - val_loss: 0.6820 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.68100\n",
      "Epoch 38/100\n",
      "877/877 [==============================] - 0s 108us/step - loss: 0.6352 - accuracy: 0.6328 - val_loss: 0.6891 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68100\n",
      "Epoch 39/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6539 - accuracy: 0.6100 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68100\n",
      "Epoch 40/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6483 - accuracy: 0.6169 - val_loss: 0.6961 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68100\n",
      "Epoch 41/100\n",
      "877/877 [==============================] - 0s 109us/step - loss: 0.6467 - accuracy: 0.6351 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.68100\n",
      "Epoch 42/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.6453 - accuracy: 0.6317 - val_loss: 0.6935 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.68100\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877/877 [==============================] - 0s 99us/step - loss: 0.6420 - accuracy: 0.6260 - val_loss: 0.6926 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.68100\n",
      "Epoch 44/100\n",
      "877/877 [==============================] - 0s 95us/step - loss: 0.6436 - accuracy: 0.6283 - val_loss: 0.6955 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.68100\n",
      "Epoch 45/100\n",
      "877/877 [==============================] - 0s 117us/step - loss: 0.6279 - accuracy: 0.6408 - val_loss: 0.6909 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.68100\n",
      "Epoch 46/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.6451 - accuracy: 0.6260 - val_loss: 0.6872 - val_accuracy: 0.5102\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.68100\n",
      "Epoch 47/100\n",
      "877/877 [==============================] - 0s 97us/step - loss: 0.6360 - accuracy: 0.6363 - val_loss: 0.6913 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.68100\n",
      "Epoch 48/100\n",
      "877/877 [==============================] - 0s 95us/step - loss: 0.6305 - accuracy: 0.6568 - val_loss: 0.6844 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.68100\n",
      "Epoch 49/100\n",
      "877/877 [==============================] - 0s 98us/step - loss: 0.6420 - accuracy: 0.6157 - val_loss: 0.6773 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.68100 to 0.67725, saving model to lolkek.hdf5\n",
      "Epoch 50/100\n",
      "877/877 [==============================] - 0s 92us/step - loss: 0.6336 - accuracy: 0.6351 - val_loss: 0.6705 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.67725 to 0.67051, saving model to lolkek.hdf5\n",
      "Epoch 51/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6310 - accuracy: 0.6397 - val_loss: 0.6790 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.67051\n",
      "Epoch 52/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.6389 - accuracy: 0.6454 - val_loss: 0.6789 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.67051\n",
      "Epoch 53/100\n",
      "877/877 [==============================] - 0s 97us/step - loss: 0.6430 - accuracy: 0.6499 - val_loss: 0.6747 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.67051\n",
      "Epoch 54/100\n",
      "877/877 [==============================] - 0s 112us/step - loss: 0.6403 - accuracy: 0.6249 - val_loss: 0.6764 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.67051\n",
      "Epoch 55/100\n",
      "877/877 [==============================] - 0s 121us/step - loss: 0.6250 - accuracy: 0.6534 - val_loss: 0.6793 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.67051\n",
      "Epoch 56/100\n",
      "877/877 [==============================] - 0s 116us/step - loss: 0.6338 - accuracy: 0.6294 - val_loss: 0.6800 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.67051\n",
      "Epoch 57/100\n",
      "877/877 [==============================] - 0s 113us/step - loss: 0.6151 - accuracy: 0.6670 - val_loss: 0.6789 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.67051\n",
      "Epoch 58/100\n",
      "877/877 [==============================] - 0s 122us/step - loss: 0.6287 - accuracy: 0.6522 - val_loss: 0.6828 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.67051\n",
      "Epoch 59/100\n",
      "877/877 [==============================] - 0s 105us/step - loss: 0.6054 - accuracy: 0.6693 - val_loss: 0.6901 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.67051\n",
      "Epoch 60/100\n",
      "877/877 [==============================] - 0s 105us/step - loss: 0.6144 - accuracy: 0.6648 - val_loss: 0.6931 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.67051\n",
      "Epoch 61/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6195 - accuracy: 0.6636 - val_loss: 0.6946 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.67051\n",
      "Epoch 62/100\n",
      "877/877 [==============================] - 0s 99us/step - loss: 0.6143 - accuracy: 0.6636 - val_loss: 0.6986 - val_accuracy: 0.4694\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.67051\n",
      "Epoch 63/100\n",
      "877/877 [==============================] - 0s 99us/step - loss: 0.6196 - accuracy: 0.6534 - val_loss: 0.7031 - val_accuracy: 0.5204\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.67051\n",
      "Epoch 64/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.6130 - accuracy: 0.6739 - val_loss: 0.6973 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.67051\n",
      "Epoch 65/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.6146 - accuracy: 0.6636 - val_loss: 0.6899 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.67051\n",
      "Epoch 66/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6211 - accuracy: 0.6568 - val_loss: 0.6808 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.67051\n",
      "Epoch 67/100\n",
      "877/877 [==============================] - 0s 111us/step - loss: 0.6190 - accuracy: 0.6750 - val_loss: 0.6787 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.67051\n",
      "Epoch 68/100\n",
      "877/877 [==============================] - 0s 108us/step - loss: 0.6092 - accuracy: 0.6864 - val_loss: 0.6806 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.67051\n",
      "Epoch 69/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.6161 - accuracy: 0.6488 - val_loss: 0.6786 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.67051\n",
      "Epoch 70/100\n",
      "877/877 [==============================] - 0s 95us/step - loss: 0.6128 - accuracy: 0.6762 - val_loss: 0.6824 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.67051\n",
      "Epoch 71/100\n",
      "877/877 [==============================] - 0s 97us/step - loss: 0.6198 - accuracy: 0.6682 - val_loss: 0.6818 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.67051\n",
      "Epoch 72/100\n",
      "877/877 [==============================] - 0s 95us/step - loss: 0.6141 - accuracy: 0.6682 - val_loss: 0.6773 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.67051\n",
      "Epoch 73/100\n",
      "877/877 [==============================] - 0s 109us/step - loss: 0.6205 - accuracy: 0.6534 - val_loss: 0.6771 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.67051\n",
      "Epoch 74/100\n",
      "877/877 [==============================] - 0s 96us/step - loss: 0.6020 - accuracy: 0.6716 - val_loss: 0.6715 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.67051\n",
      "Epoch 75/100\n",
      "877/877 [==============================] - 0s 99us/step - loss: 0.5987 - accuracy: 0.6739 - val_loss: 0.6756 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.67051\n",
      "Epoch 76/100\n",
      "877/877 [==============================] - 0s 96us/step - loss: 0.6043 - accuracy: 0.6739 - val_loss: 0.6798 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.67051\n",
      "Epoch 77/100\n",
      "877/877 [==============================] - 0s 96us/step - loss: 0.6130 - accuracy: 0.6682 - val_loss: 0.6759 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.67051\n",
      "Epoch 78/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.6054 - accuracy: 0.6739 - val_loss: 0.6703 - val_accuracy: 0.6531\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.67051 to 0.67028, saving model to lolkek.hdf5\n",
      "Epoch 79/100\n",
      "877/877 [==============================] - 0s 92us/step - loss: 0.5971 - accuracy: 0.6739 - val_loss: 0.6775 - val_accuracy: 0.6122\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.67028\n",
      "Epoch 80/100\n",
      "877/877 [==============================] - 0s 97us/step - loss: 0.6073 - accuracy: 0.6853 - val_loss: 0.6820 - val_accuracy: 0.6122\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.67028\n",
      "Epoch 81/100\n",
      "877/877 [==============================] - 0s 98us/step - loss: 0.6189 - accuracy: 0.6636 - val_loss: 0.6726 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.67028\n",
      "Epoch 82/100\n",
      "877/877 [==============================] - 0s 105us/step - loss: 0.6111 - accuracy: 0.6727 - val_loss: 0.6662 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.67028 to 0.66615, saving model to lolkek.hdf5\n",
      "Epoch 83/100\n",
      "877/877 [==============================] - 0s 99us/step - loss: 0.5934 - accuracy: 0.6853 - val_loss: 0.6680 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66615\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877/877 [==============================] - 0s 99us/step - loss: 0.6028 - accuracy: 0.6727 - val_loss: 0.6790 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66615\n",
      "Epoch 85/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.5962 - accuracy: 0.6762 - val_loss: 0.6956 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66615\n",
      "Epoch 86/100\n",
      "877/877 [==============================] - 0s 103us/step - loss: 0.6033 - accuracy: 0.6830 - val_loss: 0.6961 - val_accuracy: 0.5612\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66615\n",
      "Epoch 87/100\n",
      "877/877 [==============================] - 0s 106us/step - loss: 0.5981 - accuracy: 0.6659 - val_loss: 0.6936 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66615\n",
      "Epoch 88/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.5886 - accuracy: 0.6910 - val_loss: 0.6806 - val_accuracy: 0.5714\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.66615\n",
      "Epoch 89/100\n",
      "877/877 [==============================] - 0s 108us/step - loss: 0.5863 - accuracy: 0.6807 - val_loss: 0.6808 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66615\n",
      "Epoch 90/100\n",
      "877/877 [==============================] - 0s 131us/step - loss: 0.5926 - accuracy: 0.6796 - val_loss: 0.6737 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66615\n",
      "Epoch 91/100\n",
      "877/877 [==============================] - 0s 119us/step - loss: 0.5882 - accuracy: 0.6784 - val_loss: 0.6744 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66615\n",
      "Epoch 92/100\n",
      "877/877 [==============================] - 0s 115us/step - loss: 0.5829 - accuracy: 0.7081 - val_loss: 0.6736 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66615\n",
      "Epoch 93/100\n",
      "877/877 [==============================] - 0s 124us/step - loss: 0.5842 - accuracy: 0.7035 - val_loss: 0.6788 - val_accuracy: 0.6020\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66615\n",
      "Epoch 94/100\n",
      "877/877 [==============================] - 0s 105us/step - loss: 0.5891 - accuracy: 0.6705 - val_loss: 0.6808 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66615\n",
      "Epoch 95/100\n",
      "877/877 [==============================] - 0s 111us/step - loss: 0.5890 - accuracy: 0.6807 - val_loss: 0.6720 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.66615\n",
      "Epoch 96/100\n",
      "877/877 [==============================] - 0s 107us/step - loss: 0.5717 - accuracy: 0.7104 - val_loss: 0.6670 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66615\n",
      "Epoch 97/100\n",
      "877/877 [==============================] - 0s 104us/step - loss: 0.5949 - accuracy: 0.6670 - val_loss: 0.6673 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66615\n",
      "Epoch 98/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.5871 - accuracy: 0.6864 - val_loss: 0.6646 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.66615 to 0.66458, saving model to lolkek.hdf5\n",
      "Epoch 99/100\n",
      "877/877 [==============================] - 0s 100us/step - loss: 0.5791 - accuracy: 0.7035 - val_loss: 0.6653 - val_accuracy: 0.6020\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66458\n",
      "Epoch 100/100\n",
      "877/877 [==============================] - 0s 101us/step - loss: 0.5742 - accuracy: 0.6899 - val_loss: 0.6711 - val_accuracy: 0.5816\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66458\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, \n",
    "          nb_epoch = 100, \n",
    "          batch_size = 128, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[reduce_lr, checkpointer],\n",
    "          shuffle=True)\n",
    "\n",
    "model.load_weights(\"lolkek.hdf5\")\n",
    "pred = model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Uz--c9TtIDVx",
    "outputId": "08598473-2185-4ed3-8153-eac14f9f396d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71428571 0.28571429]\n",
      " [0.46938776 0.53061224]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "C = confusion_matrix([np.argmax(y) for y in Y_test], [np.argmax(y) for y in pred])\n",
    "\n",
    "print(C / C.astype(np.float).sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsIU8nUmNIF7"
   },
   "source": [
    "which shows that we predict “UP” movement with 70% of accuracy and “DOWN” with 51% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Iyn9Eo0ZIH2-",
    "outputId": "c1ac3d82-8a8d-4238-a2eb-a471607257cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0] [0.6338408 0.3661592]\n",
      "[1 0] [0.7959021  0.20409791]\n",
      "[1 0] [0.666735   0.33326498]\n",
      "[1 0] [0.5082663  0.49173376]\n",
      "[0 1] [0.53186685 0.46813312]\n",
      "[0 1] [0.6448086  0.35519144]\n",
      "[0 1] [0.5580725  0.44192743]\n",
      "[0 1] [0.56525654 0.43474352]\n",
      "[0 1] [0.5619171  0.43808287]\n",
      "[1 0] [0.574535  0.4254649]\n",
      "[1 0] [0.67273515 0.3272648 ]\n",
      "[1 0] [0.53330415 0.4666958 ]\n",
      "[0 1] [0.31873715 0.68126285]\n",
      "[0 1] [0.33054277 0.6694572 ]\n",
      "[0 1] [0.52954394 0.4704561 ]\n",
      "[1 0] [0.6253914  0.37460855]\n",
      "[0 1] [0.62574345 0.37425652]\n",
      "[1 0] [0.5611033  0.43889672]\n",
      "[1 0] [0.6614097  0.33859032]\n",
      "[1 0] [0.6438691 0.3561309]\n",
      "[1 0] [0.6675757  0.33242428]\n",
      "[1 0] [0.5431795 0.4568205]\n",
      "[1 0] [0.22493379 0.7750662 ]\n",
      "[1 0] [0.41223347 0.58776647]\n",
      "[0 1] [0.5818549  0.41814503]\n",
      "[0 1] [0.2874361 0.712564 ]\n",
      "[1 0] [0.18792698 0.81207305]\n",
      "[0 1] [0.3240721 0.6759279]\n",
      "[0 1] [0.74137646 0.2586235 ]\n",
      "[1 0] [0.69114697 0.30885312]\n",
      "[1 0] [0.3727069  0.62729317]\n",
      "[0 1] [0.31318018 0.68681985]\n",
      "[0 1] [0.33845702 0.661543  ]\n",
      "[0 1] [0.22130182 0.7786981 ]\n",
      "[1 0] [0.331383 0.668617]\n",
      "[1 0] [0.5955081  0.40449196]\n",
      "[1 0] [0.6513317  0.34866825]\n",
      "[1 0] [0.46567824 0.5343218 ]\n",
      "[0 1] [0.50268656 0.4973134 ]\n",
      "[0 1] [0.49534908 0.5046509 ]\n",
      "[1 0] [0.5089613  0.49103868]\n",
      "[1 0] [0.5449495  0.45505056]\n",
      "[0 1] [0.42517614 0.57482386]\n",
      "[0 1] [0.3542381  0.64576185]\n",
      "[1 0] [0.3558397  0.64416033]\n",
      "[1 0] [0.41812176 0.5818783 ]\n",
      "[0 1] [0.52416855 0.4758315 ]\n",
      "[1 0] [0.50035244 0.49964756]\n",
      "[1 0] [0.27086204 0.72913796]\n",
      "[1 0] [0.4547731 0.545227 ]\n",
      "[1 0] [0.49319124 0.5068088 ]\n",
      "[0 1] [0.39283168 0.6071683 ]\n",
      "[0 1] [0.37460396 0.625396  ]\n",
      "[1 0] [0.59674007 0.40325987]\n",
      "[1 0] [0.71699697 0.28300297]\n",
      "[1 0] [0.71140635 0.2885936 ]\n",
      "[1 0] [0.7591608  0.24083917]\n",
      "[1 0] [0.5904425  0.40955755]\n",
      "[1 0] [0.37292856 0.62707144]\n",
      "[0 1] [0.435764 0.564236]\n",
      "[0 1] [0.45716923 0.54283077]\n",
      "[0 1] [0.39660573 0.60339427]\n",
      "[0 1] [0.31765738 0.6823426 ]\n",
      "[0 1] [0.37909266 0.6209074 ]\n",
      "[0 1] [0.4520522  0.54794776]\n",
      "[0 1] [0.44060132 0.55939865]\n",
      "[0 1] [0.4306761  0.56932384]\n",
      "[1 0] [0.63532966 0.36467037]\n",
      "[1 0] [0.77341276 0.22658725]\n",
      "[1 0] [0.77422446 0.22577558]\n",
      "[1 0] [0.6644423  0.33555767]\n",
      "[0 1] [0.5860476  0.41395238]\n",
      "[0 1] [0.47708556 0.5229144 ]\n",
      "[1 0] [0.50012493 0.49987516]\n",
      "[0 1] [0.54092354 0.45907637]\n",
      "[1 0] [0.41693127 0.5830688 ]\n",
      "[0 1] [0.3056492 0.6943508]\n",
      "[0 1] [0.31853127 0.6814688 ]\n",
      "[0 1] [0.32748786 0.6725121 ]\n",
      "[1 0] [0.42409068 0.57590926]\n",
      "[1 0] [0.5957443  0.40425566]\n",
      "[0 1] [0.68524635 0.31475368]\n",
      "[0 1] [0.71364117 0.28635883]\n",
      "[0 1] [0.5354076 0.4645924]\n",
      "[0 1] [0.55893356 0.44106638]\n",
      "[0 1] [0.4561958  0.54380417]\n",
      "[0 1] [0.4703465  0.52965343]\n",
      "[0 1] [0.5833739  0.41662613]\n",
      "[0 1] [0.67146826 0.3285317 ]\n",
      "[0 1] [0.6345172  0.36548284]\n",
      "[0 1] [0.6763856 0.3236144]\n",
      "[1 0] [0.7125066  0.28749344]\n",
      "[1 0] [0.6375837 0.3624163]\n",
      "[1 0] [0.5176347  0.48236534]\n",
      "[0 1] [0.6195102  0.38048977]\n",
      "[1 0] [0.5144128 0.4855872]\n",
      "[1 0] [0.5852276  0.41477236]\n",
      "[0 1] [0.6596882  0.34031186]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "     print(Y_test[i], pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "N8-qIFnKIQQd",
    "outputId": "621f7274-c782-46ea-c4ea-fe21915d5c45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "simple_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (TS_sample2)",
   "language": "python",
   "name": "ts_sample2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
